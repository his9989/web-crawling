{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 키워드 검색해서 카페 관련 글 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "\n",
    "from konlpy.tag import Twitter\n",
    "from collections import Counter\n",
    "\n",
    "def get_tags(text,ntags=50):\n",
    "    spliter = Twitter()\n",
    "    nouns = spliter.nouns(text)\n",
    "    count = Counter(nouns)\n",
    "    return_list = list()\n",
    "    for n,c in count.most_common(ntags):\n",
    "        temp = {'tag':n ,'count':c}\n",
    "        return_list.append(temp)\n",
    "    return return_list\n",
    "\n",
    "def navercafe(word,startpage):\n",
    "    client_id = \"a27TLM5yHm4HRAEaZUl1\"\n",
    "    client_secret = \"_cErdfce8o\"\n",
    "    encText = urllib.parse.quote(word)\n",
    "    #startpage = urllib.parse.quoto(str((startpage*100)-99))\n",
    "    url = \"https://openapi.naver.com/v1/search/cafearticle?query=\" + encText+\"&display=100&sort=sim&start=\"+str((startpage*100)-99)\n",
    "    # url = \"https://openapi.naver.com/v1/search/blog.xml?query=\" + encText # xml 결과\n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header(\"X-Naver-Client-Id\", client_id)\n",
    "    request.add_header(\"X-Naver-Client-Secret\", client_secret)\n",
    "    response = urllib.request.urlopen(request)\n",
    "    rescode = response.getcode()\n",
    "\n",
    "    if (rescode == 200):\n",
    "        response_body = response.read()\n",
    "      # print(response_body.decode('utf-8'))\n",
    "        temp = response_body.decode('utf-8')\n",
    "        jsondata = json.loads(temp)\n",
    "\n",
    "        result = list()\n",
    "        for jtemp in range(100):\n",
    "            tempdata = jsondata['items'][jtemp]\n",
    "            lastdata = tempdata['description']\n",
    "            lastdata = re.sub('<b>','',lastdata)\n",
    "            lastdata = re.sub('</b>', '', lastdata)\n",
    "            result.append(lastdata)\n",
    "            #print(tempdata[\"description\"]+'\\n')\n",
    "\n",
    "        return result\n",
    "    else:\n",
    "        print(\"Error Code:\" + rescode)\n",
    "\n",
    "with open('cafe.csv','w',encoding='utf-8') as txtfile:\n",
    "    writer = csv.writer(txtfile,delimiter='@')\n",
    "\n",
    "\n",
    "    textdata = ''\n",
    "\n",
    "\n",
    "    inputdata = input()\n",
    "    print(\"==========================\", inputdata)\n",
    "    for temp in range(10):\n",
    "        print(navercafe(inputdata, temp + 1))\n",
    "        lastdata = navercafe(inputdata, temp + 1)\n",
    "        for tempdata in lastdata:\n",
    "            textdata += tempdata\n",
    "        writer.writerow(lastdata)\n",
    "\n",
    "    print(get_tags(textdata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 무슨 코든지 기억은 안나네.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def naver_m_blog_search(q, n):\n",
    "    \n",
    "    url = 'https://search.naver.com/search.naver'\n",
    "    \n",
    "    headers = {\n",
    " 'User-Agent': ('Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 '\n",
    " '(KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'),\n",
    "    'Referer': 'https://search.naver.com/search.naver', }\n",
    "    time = []\n",
    "    i =0\n",
    "    for page in range(1,n):\n",
    "        params = {'query': q,\n",
    "                  'where': 'post',\n",
    "                  'start': (page-1)*10 +1,}\n",
    "        \n",
    "        response = requests.get(url, params=params, headers=headers)\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "\n",
    "        for tag in soup.select('dd.txt_inline'):\n",
    "            time.append(tag.text)\n",
    "            \n",
    "        for tag  in soup.select('.sh_blog_title'):\n",
    "            blog_url = str(tag['href'])\n",
    "            blog_urls = blog_url.split('/')\n",
    "\n",
    "            if blog_urls[2] == 'blog.naver.com':\n",
    "                edit_url = blog_url[0:7]+'m.'+blog_url[7:]\n",
    "            else:\n",
    "                edit_url = \"https://m.blog.naver.com/\" + blog_urls[2].split('.')[0] + '/'+blog_urls[3]\n",
    "            \n",
    "            html2 = requests.get(edit_url, headers=headers).text\n",
    "            soup2 = BeautifulSoup(html2,'html.parser')\n",
    "            \n",
    "            print('제목 : ', tag.text)\n",
    "            print('날짜 : ', time[i])\n",
    "            print('주소 : ', edit_url)\n",
    "            print('본문내용 : ')\n",
    "            \n",
    "            for tag in soup2.select('.se_paragraph p.se_textarea'):\n",
    "                print(tag.text)\n",
    "\n",
    "            print(\"-------------------\\n\")\n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "# naver_m_blog_search(\"검색어\", \"페이지 횟수 1 단위당 10개\")\n",
    "naver_m_blog_search('치킨',3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 카페내에서 게시글 긁어오는 방법. 블로그로 변환 요망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import 및 접속/로그인\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.parse import quote\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "driver = webdriver.Chrome('chromedriver.exe') # jpynd 파일과 같은 디렉토리에 있는 chromedriver.exe를 실행.\n",
    "soup = bs(driver.page_source, 'html.parser')  # beautifulsoup 관련 프레임 변화 코드인 듯. 이해 부족\n",
    "driver.implicitly_wait(3)                     # ..\n",
    "\n",
    "driver.get('https://nid.naver.com/nidlogin.login') # 네이버 로그인 url을 접속하는 코드\n",
    "\n",
    "driver.find_element_by_name('id').send_keys('his9989')       # 입력한 정보를 로그인 화면의 ID 란에 입력\n",
    "driver.find_element_by_name('pw').send_keys('gksdlstn9566@') # 입력한 정보를 로그인 화면의 PW 란에 입력\n",
    "driver.find_element_by_css_selector('#frmNIDLogin > fieldset > input').click() # 로그인 버튼 클릭\n",
    "\n",
    "base_url = 'http://cafe.naver.com/byungs94'   ## 수정 필요 항목. 크롤링 하고 싶은 카페의 main 화면 html 주소를 복사\n",
    "article_urls=[]   ## 게시글의 주소를 넣는 배열 초기화\n",
    "how_many_page=int(input(\"몇 페이지를 크롤링하시겠습니까? : \"))\n",
    "                  ## how_many_page라는 변수를 이용하여 몇 개의 page를 크롤링 할 것인지 input\n",
    "                  \n",
    "for i in range(how_many_page):  ## 입력한 크롤링 페이지 수 만큼 반복\n",
    "\n",
    "    driver.get(base_url+'/ArticleSearchList.nhn?search.clubid=13276223&search.media=0&search.searchdate=all&userDisplay=15&search.option=0&search.sortBy=date&search.searchBy=1&search.query=%C6%ED%BD%C4&search.viewtype=title&search.page={}'.format(i+1))\n",
    "                            ## 직접 추출할 수 있지만, 빠른 과제 수행을 위해 이렇게 처리하였습니다.\n",
    "                            ## 코드 형식 상, 마지막에 숫자(1,2,3,4,...)가 기입되어 있는데, 이 부분만 {}으로 대체해줍니다.\n",
    "                            ## 카페에서 키워드를 검색한 뒤 F12을 누르고, 페이지를 표시하는 하단의 숫자를 클릭하면 확인 가능합니다.\n",
    "\n",
    "    css_selector = \"input#topLayerQueryInput\"\n",
    "                  ## beautifulsoup을 이용한, 코드인 듯 한데, css에 대한 이해가 부족하여 완전히 이해하지는 못했습니다.\n",
    "    search_inpupt_element = driver.find_element_by_css_selector(\"input#topLayerQueryInput\")\n",
    "                  ## 정보 추출을 위한, 프레임을 변화시키는 코드라고 추측합니다.\n",
    "    iframe_element = driver.find_element_by_css_selector(\"iframe#cafe_main\")\n",
    "                  ## ...\n",
    "    driver.switch_to_frame(iframe_element)\n",
    "                  ## ...\n",
    "\n",
    "    article_list = driver.find_elements_by_css_selector('span.aaa > a.m-tcol-c')\n",
    "                  ## 키워드를 검색한 뒤, 나오는 15개의 게시글 목록의 주소를 크롤링합니다.\n",
    "                  ## span.aaa > a.m-tcol-c를 이용하여 주소가 포함된 코드를 가져온 뒤\n",
    "    article_urls = [ i.get_attribute('href') for i in article_list ]\n",
    "                  ## get_attribute('href')를 이용하여, 코드에서 href 이름으로 저장되어 있는 게시글의 주소를\n",
    "                  ## 초기화한 article_urls 배열에 저장합니다.\n",
    "\n",
    "    i+=1                             ## 페이지 반복 조회를 위한 i+=1을 수행합니다.\n",
    "    print(\"**crawlin page...%d\" %i)  ## 몇 번째 page를 크롤링하는지 현황을 보여줍니다.\n",
    "    num = 1                          ## 몇 번째 게시물인지 알기 위한 변수 초기화\n",
    "    \n",
    "    for article in article_urls:\n",
    "        print(article)\n",
    "    \n",
    "    for article in article_urls:  ## article이라는 변수를 사용하여 \n",
    "        try:\n",
    "            driver.get(article)       ## article_urls 배열에 저장된 게시글에 하나씩 방문합니다.\n",
    "\n",
    "            driver.switch_to_frame('cafe_main')           ## beautifulsoup을 이용하여 article도 frame 변환을 해줍니다.\n",
    "            soup = bs(driver.page_source, 'html.parser')  ## ...\n",
    "\n",
    "            title = soup.select('div.tit-box span.b')[0].get_text() ## 게시글의 제목을 title이라는 변수로 저장합니다.\n",
    "            content_tags = soup.select('#tbody')[0].select('p')     \n",
    "                      ## #tbody-p로 분류되어 있는 게시글 내용을 content_tags라는 배열로 가져옵니다.\n",
    "            content = ' '.join([ tags.get_text() for tags in content_tags ])\n",
    "                      ## 게시글 내용 전체를 띄어쓰기 하나로 구분하여 전체 게시글로 합칩니다.\n",
    "                      ## 이는 하나의 게시글 안에 여러 문단으로 떨어져 있는 게시글을 하나의 변수로 저장하는 역할을 수행합니다.\n",
    "\n",
    "            reply_tags = soup.select('#cmt_list')[0].select('span.comm_body')  ## 댓글 목록을 저장하는 역할을 수행합니다.\n",
    "            reply = ' '.join([tags.get_text() for tags in reply_tags ]) ## 전체 댓글은 reply라는 변수를 이용하여 하나의 변수로 저장합니다.\n",
    "\n",
    "            print('%d..' %num, end=' ')  ## page 내에서 몇 번째 게시글인지 표시합니다.\n",
    "            print(title, end=\" : \")      ## 제목을 표시합니다.\n",
    "            print(content)               ## 게시글 내용을 표시합니다.\n",
    "            print(reply)                 ## 게시글 댓글을 표시합니다.\n",
    "            num += 1                     ## 페이지 내의 게시글 반복 조회를 위한 num+=1을 수행합니다.\n",
    "        except:\n",
    "            print(\"%d..\" %num, end=\" \")\n",
    "            print(\"가입 회원에게만 공개입니다.\")\n",
    "            num+=1\n",
    "                \n",
    "    print('\\n')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
